---
title: "HC2 Validation: Coverage Under Heteroskedasticity and Universal Lookup Table"
author: "Quantitative Methods"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    number_sections: yes
---

# Purpose

This notebook implements the heteroskedastic validation stage, establishing the empirical relationship between the inferential score $ S_{\mathrm{Inf}} $ and nominal coverage loss under model misspecification. The workflow:

1. Loads the selected HC estimator from null calibration (Notebook 1).
2. Simulates heteroskedastic DGPs with controlled misspecification levels across a grid of sample sizes, heteroskedasticity strengths, effect sizes, and baseline noise levels.
3. Evaluates the power of the inferential score $S_{\mathrm{Inf}}$ to detect heteroskedasticity.
4. Demonstrates the N-dependence of $S_{\mathrm{Inf}}$ (behaving like a p-value) and motivates the need for a reliability score.
5. Introduces the **Reliability Score** ($S_{\mathrm{Rel}}$ or `sr_ratio_adj`) and demonstrates its approximate N-invariance.
6. Benchmarks $S_{\mathrm{Inf}}$ and $S_{\mathrm{Rel}}$ against standard heteroskedasticity tests (Breusch-Pagan, White).
7. Constructs the universal lookup table mapping $S_{\mathrm{Rel}}$ to expected coverage loss.

---

# Section 1: Setup and Load Notebook 1 Results

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Load Support Functions and Configuration

```{r source-files}
# Source modular R support functions
# R/00_config.R handles package loading, parallel setup, and global constants
source("R/00_config.R", local = TRUE)              
source("R/10_dgp_and_fits.R", local = TRUE)        # Data generation and fitting
source("R/20_metrics.R", local = TRUE)              # Metric computations
source("R/40_hetero_sims.R", local = TRUE)          # Heteroskedastic simulation grid
source("R/50_invariance_and_lookup.R", local = TRUE) # Invariance and lookup table construction
source("R/60_tables_and_plots.R", local = TRUE)     # Presentation utilities
```

```{r setup-paths}
BASE_PATH <- getwd()

# Note: Global seed is set in R/00_config.R and will be used for all simulations
# The seed is automatically propagated to parallel workers via furrr_options(seed = TRUE)

# Create results directory if necessary
if (!dir.exists("results")) {
  dir.create("results", recursive = TRUE)
}
```

## Load Results from Null Calibration

```{r load-null-results}
# Retrieve HC type selected in null calibration
best_hc <- readRDS("results/best_hc.rds")
scaling_results <- readRDS("results/scaling_results.rds")

cat(sprintf("Selected HC estimator: %s\n", best_hc))
cat(sprintf("Stability range at optimal scaling: %.6f\n\n", 
            scaling_results[hc_type == best_hc, stability_range_inf]))
```

---

# Section 2: Heteroskedastic Simulation Grid

Define the experimental grid for heteroskedastic validation. The heteroskedastic DGP is:
$$ Y_i = 1 + \beta_x \cdot X_i + \varepsilon_i, \quad \varepsilon_i \sim N(0, \sigma_i^2) $$
where the variance is:
$$ \sigma_i^2 = \sigma_0^2 + \lambda \cdot X_i^2 $$
Here:
- $\beta_x$ controls the effect size of the predictor.
- $\sigma_0$ controls the baseline noise level (homoskedastic component).
- $\lambda$ (heteroskedasticity strength) controls how strongly the variance increases with $X^2$.

```{r hetero-config}
# Simulation grid: sample sizes, heteroskedasticity strengths, and replicates per cell
# Note: These are defined in R/00_config.R and loaded automatically.
# We display them here for confirmation.

total_hetero <- length(N_GRID_HETERO) * 
                length(HETERO_STRENGTH_GRID) * 
                length(BETA_X_GRID) * 
                length(SIGMA0_GRID) * 
                N_SIMS_PER_CELL_HETERO

cat(sprintf("Simulation grid configuration:\n"))
cat(sprintf("  Sample sizes (N):           %s\n", paste(N_GRID_HETERO, collapse = ", ")))
cat(sprintf("  Hetero strength (λ):        %d levels from %.1f to %.1f\n", 
            length(HETERO_STRENGTH_GRID), min(HETERO_STRENGTH_GRID), max(HETERO_STRENGTH_GRID)))
cat(sprintf("  Beta X values:              %s\n", paste(BETA_X_GRID, collapse = ", ")))
cat(sprintf("  Sigma0 values:              %s\n", paste(SIGMA0_GRID, collapse = ", ")))
cat(sprintf("  Replicates per cell:        %d\n", N_SIMS_PER_CELL_HETERO))
cat(sprintf("  Total simulations:          %d\n", total_hetero))
cat(sprintf("  HC estimator:               %s\n\n", best_hc))
```

---

# Section 3: Heteroskedastic Simulation and Metrics

Execute the full heteroskedastic simulation grid. For each cell (N, λ, β, σ0), generate replicates, compute 95% confidence intervals using both classical and HC standard errors, and record coverage outcomes.

The simulation engine `run_hetero_sim_grid` (in `R/40_hetero_sims.R`) computes:
- `sr_inf`: Inferential Score $S_{\mathrm{Inf}} = \text{SE}_{\text{robust}}/\text{SE}_{\text{classic}} - 1$
- `sr_inf_adj`: Scaled Inferential Score $S_{\mathrm{Inf}}^{*} = \sqrt{N} \cdot S_{\mathrm{Inf}}$
- `coverage`: Classical CI coverage (using $\text{SE}_{\text{classic}}$)

**Note:** Ratio-based metrics ($S_{\mathrm{Rel}}$) are computed post-hoc in Section 5 after establishing the N-dependence problem.

```{r run-hetero-sims}
# Note: Random seed already set in R/00_config.R and propagated to parallel workers
# via furrr_options(seed = TRUE) for full reproducibility

hetero_sim_results <- run_hetero_sim_grid(
  N_grid = N_GRID_HETERO,
  hetero_strength_grid = HETERO_STRENGTH_GRID,
  beta_x_grid = BETA_X_GRID,
  sigma0_grid = SIGMA0_GRID,
  n_sims_per_cell = N_SIMS_PER_CELL_HETERO,
  hc_type = best_hc,
  verbose = TRUE
)

# Save raw simulation results
save_rds(hetero_sim_results, "hetero_sim_results.rds")
```

## Summary of Simulation Results

We summarize the heteroskedastic simulation results, focusing on the behavior of the **Inferential Score** $S_{\mathrm{Inf}}$ across the experimental grid before proceeding to formal analysis.

```{r simulation-summary}
# Basic summary statistics
cat("Simulation Results Summary\n")
cat("=" |> rep(60) |> paste(collapse = ""), "\n\n")

cat(sprintf("Total simulations:        %d\n", nrow(hetero_sim_results)))
cat(sprintf("Sample sizes (N):         %s\n", paste(sort(unique(hetero_sim_results$N)), collapse = ", ")))
cat(sprintf("Hetero strengths (λ):     %d levels\n", length(unique(hetero_sim_results$hetero_strength))))
cat(sprintf("Effect sizes (β_x):       %s\n", paste(unique(hetero_sim_results$beta_x), collapse = ", ")))
cat(sprintf("Baseline noise (σ0):      %s\n", paste(unique(hetero_sim_results$sigma0), collapse = ", ")))

# Summary of Inferential Score
cat("\n\nInferential Score (sr_inf) Summary:\n")
cat("-" |> rep(40) |> paste(collapse = ""), "\n")
print(summary(hetero_sim_results$sr_inf))

# Summary of Scaled Inferential Score
cat("\n\nScaled Inferential Score (sr_inf_adj = sqrt(N) * sr_inf) Summary:\n")
cat("-" |> rep(40) |> paste(collapse = ""), "\n")
print(summary(hetero_sim_results$sr_inf_adj))
```

### Distribution of the Inferential Score

We examine the distribution of $S_{\mathrm{Inf}}$ across all simulations and by heteroskedasticity level.

```{r sinf-distribution, fig.width=10, fig.height=4}
# Overall distribution of S_Inf
p1 <- ggplot(hetero_sim_results, aes(x = sr_inf)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "white", alpha = 0.7) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Distribution of Inferential Score (All Simulations)",
    x = expression(S[Inf]),
    y = "Count"
  ) +
  theme_minimal()

# Distribution by heteroskedasticity strength (subset for clarity)
hetero_subset <- hetero_sim_results[hetero_strength %in% c(0, 0.5, 1.0, 1.5, 2.0)]
p2 <- ggplot(hetero_subset, aes(x = sr_inf, fill = as.factor(hetero_strength))) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray40") +
  labs(
    title = "Inferential Score by Heteroskedasticity Level",
    x = expression(S[Inf]),
    y = "Density",
    fill = "Lambda"
  ) +
  theme_minimal()

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

**Key Observations:**
- Under the null ($\lambda = 0$), $S_{\mathrm{Inf}}$ is centered near zero.
- As heteroskedasticity increases, $S_{\mathrm{Inf}}$ shifts positive (robust SE > classic SE).
- The distribution widens with increasing $\lambda$, reflecting greater variability under misspecification.

# Section 4: Power and N-Dependence of the Inferential Score $S_{\mathrm{Inf}}$

We evaluate the inferential score $S_{\mathrm{Inf}}$ as a test statistic for heteroskedasticity. We define:
$$ T_{S_{\mathrm{Inf}}} = \sqrt{N} \cdot S_{\mathrm{Inf}} $$
Under the homoskedastic null, $T_{S_{\mathrm{Inf}}}$ is approximately standard normal (for HC2). We reject the null of homoskedasticity if $T_{S_{\mathrm{Inf}}} > 1.645$ (one-sided $\alpha=0.05$).

## 4.1 Power Analysis

```{r power-analysis}
# Compute T_SInf (which is sr_inf_adj in our nomenclature)
hetero_sim_results[, T_SInf := sr_inf_adj]

# Define rejection indicator
hetero_sim_results[, reject_null := as.integer(T_SInf > 1.645)]

# Aggregate rejection rates by N and hetero_strength (averaging over beta_x and sigma0)
power_summary <- hetero_sim_results[, .(
  rejection_rate = mean(reject_null)
), by = .(N, hetero_strength)]

# Plot Power Curves
ggplot(power_summary, aes(x = hetero_strength, y = rejection_rate, color = as.factor(N))) +
  geom_line(size = 1) +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "gray50") +
  labs(
    title = "Power of Inferential Score to Detect Heteroskedasticity",
    subtitle = "Rejection Rate of T_SInf > 1.645 (alpha = 0.05)",
    x = "Heteroskedasticity Strength (lambda)",
    y = "Rejection Rate (Power)",
    color = "Sample Size (N)"
  ) +
  theme_minimal()
```

**Interpretation:**
- At $\lambda = 0$ (null), the rejection rate is close to 0.05, confirming the test is well-calibrated.
- As $\lambda$ increases, power increases.
- Crucially, for a fixed $\lambda > 0$, power increases with $N$. This confirms that $S_{\mathrm{Inf}}$ behaves like a test statistic: it detects the *existence* of misspecification with increasing certainty as sample size grows.

## 4.2 Formal Test of Parameter Effects (Multilevel ANOVA)

To formally establish which parameters affect $T_{S_{\mathrm{Inf}}}$, we fit a full factorial ANOVA with all experimental factors: sample size (N), heteroskedasticity strength (λ), effect size (β_x), and baseline noise (σ0).

**Note:** With ~300,000 observations, virtually all effects will be statistically significant ($p < 0.001$). We therefore focus on **effect sizes** (η², eta-squared) to assess practical significance:
- η² < 0.01: negligible effect
- η² ∈ [0.01, 0.06): small effect  
- η² ∈ [0.06, 0.14): medium effect
- η² ≥ 0.14: large effect

```{r n-dependence-anova}
# Use RAW simulation data (not aggregated) to preserve within-cell variance
# Convert to factors for ANOVA
hetero_sim_results[, `:=`(
  N_factor = as.factor(N),
  lambda_factor = as.factor(hetero_strength),
  beta_factor = as.factor(beta_x),
  sigma_factor = as.factor(sigma0)
)]

# Fit full factorial ANOVA on raw data
# Note: With ~300k observations, p-values will all be < 0.001
# Focus on eta-squared for practical significance
anova_sinf_full <- aov(T_SInf ~ N_factor * lambda_factor * beta_factor * sigma_factor, 
                       data = hetero_sim_results)
anova_summary_full <- summary(anova_sinf_full)

cat("Full Factorial ANOVA: T_SInf ~ N × λ × β_x × σ0\n")
cat("=" |> rep(70) |> paste(collapse = ""), "\n")
cat(sprintf("Total observations: %d\n\n", nrow(hetero_sim_results)))

# Compute eta-squared for each effect
ss_total <- sum(anova_summary_full[[1]]$`Sum Sq`)
anova_table <- anova_summary_full[[1]]
anova_table$eta_sq <- anova_table$`Sum Sq` / ss_total
anova_table$eta_sq_pct <- round(anova_table$eta_sq * 100, 2)

# Display results with effect sizes
cat("Effect Sizes (η² = Sum Sq / Total Sum Sq):\n")
cat("-" |> rep(70) |> paste(collapse = ""), "\n")
cat(sprintf("%-40s %12s %10s %10s\n", "Effect", "F value", "η²", "η² (%)"))
cat("-" |> rep(70) |> paste(collapse = ""), "\n")

for (i in 1:(nrow(anova_table) - 1)) {  # Exclude Residuals row
  effect_name <- rownames(anova_table)[i]
  f_val <- anova_table$`F value`[i]
  eta_sq <- anova_table$eta_sq[i]
  eta_pct <- anova_table$eta_sq_pct[i]
  
  # Effect size interpretation
  size_interp <- ifelse(eta_sq >= 0.14, "LARGE",
                        ifelse(eta_sq >= 0.06, "Medium",
                               ifelse(eta_sq >= 0.01, "Small", "Negligible")))
  
  cat(sprintf("%-40s %12.1f %10.4f %9.2f%%  [%s]\n", 
              effect_name, f_val, eta_sq, eta_pct, size_interp))
}

# Residuals
cat("-" |> rep(70) |> paste(collapse = ""), "\n")
resid_idx <- nrow(anova_table)
cat(sprintf("%-40s %12s %10.4f %9.2f%%\n", 
            "Residuals", "-", 
            anova_table$eta_sq[resid_idx], 
            anova_table$eta_sq_pct[resid_idx]))
```

```{r anova-interpretation}
# Extract key effects for interpretation
n_eta <- anova_table["N_factor", "eta_sq"]
lambda_eta <- anova_table["lambda_factor", "eta_sq"]
n_lambda_eta <- anova_table["N_factor:lambda_factor", "eta_sq"]

# Check for beta and sigma effects
beta_eta <- ifelse("beta_factor" %in% rownames(anova_table), 
                   anova_table["beta_factor", "eta_sq"], NA)
sigma_eta <- ifelse("sigma_factor" %in% rownames(anova_table), 
                    anova_table["sigma_factor", "eta_sq"], NA)

cat("\n\nKey Findings:\n")
cat("=" |> rep(50) |> paste(collapse = ""), "\n\n")

cat(sprintf("1. Sample Size (N):           η² = %.4f (%.1f%% of variance)\n", 
            n_eta, n_eta * 100))
cat(sprintf("2. Heteroskedasticity (λ):    η² = %.4f (%.1f%% of variance)\n", 
            lambda_eta, lambda_eta * 100))
cat(sprintf("3. N × λ Interaction:         η² = %.4f (%.1f%% of variance)\n", 
            n_lambda_eta, n_lambda_eta * 100))

if (!is.na(beta_eta)) {
  cat(sprintf("4. Effect Size (β_x):         η² = %.4f (%.1f%% of variance)\n", 
              beta_eta, beta_eta * 100))
}
if (!is.na(sigma_eta)) {
  cat(sprintf("5. Baseline Noise (σ0):       η² = %.4f (%.1f%% of variance)\n", 
              sigma_eta, sigma_eta * 100))
}

cat("\n\nInterpretation:\n")
cat("-" |> rep(50) |> paste(collapse = ""), "\n")

if (n_eta >= 0.01) {
  cat("→ N explains a meaningful portion of T_SInf variance.\n")
  cat("→ The inferential score is NOT sample-size invariant.\n")
}
if (lambda_eta >= 0.06) {
  cat("→ λ has a substantial effect (as expected for a heteroskedasticity diagnostic).\n")
}
if (n_lambda_eta >= 0.01) {
  cat("→ The N × λ interaction is meaningful: N-dependence varies with misspecification level.\n")
}
if (!is.na(beta_eta) && beta_eta < 0.01) {
  cat("→ Effect size (β_x) has negligible impact—score is effect-size invariant.\n")
}
if (!is.na(sigma_eta) && sigma_eta < 0.01) {
  cat("→ Baseline noise (σ0) has negligible impact—score is noise-level invariant.\n")
}

cat("\n→ CONCLUSION: T_SInf depends substantially on N, making it unsuitable\n")
cat("   as a direct measure of misspecification SEVERITY across sample sizes.\n")
```

## 4.3 Visualization of N-Dependence

Having established statistically significant N-dependence, we now visualize how $T_{S_{\mathrm{Inf}}}$ scales with sample size for fixed levels of heteroskedasticity.

```{r n-dependence-sinf-plot}
# Subset to representative parameters (beta_x = 0.5, sigma0 = 1.0)
# Use ALL available lambda values from HETERO_STRENGTH_GRID (0, 0.2, 0.4, ..., 2.0)
subset_params <- hetero_sim_results[beta_x == 0.5 & sigma0 == 1.0]

# Aggregate mean T_SInf
sinf_n_summary <- subset_params[, .(
  mean_sinf = mean(sr_inf),
  mean_T_sinf = mean(T_SInf)
), by = .(N, hetero_strength)]

# Plot Mean T_SInf vs log(N) for all lambda levels
ggplot(sinf_n_summary, aes(x = N, y = mean_T_sinf, color = as.factor(hetero_strength))) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  scale_x_log10() +
  scale_color_viridis_d(option = "plasma", end = 0.9) +
  labs(
    title = "N-Dependence of Test Statistic T_SInf",
    subtitle = "Mean T_SInf increases with N for fixed misspecification level",
    x = "Sample Size (log scale)",
    y = "Mean T_SInf",
    color = "λ"
  ) +
  theme_minimal() +
  theme(legend.position = "right")
```

**Conclusion from Section 4:**
The inferential score $S_{\mathrm{Inf}}$ (and its scaled form $T_{S_{\mathrm{Inf}}}$) exhibits strong N-dependence. While this is desirable for a *test statistic* (more power with larger N), it makes $S_{\mathrm{Inf}}$ unsuitable as a direct measure of misspecification *severity* that can be mapped to coverage loss across different sample sizes.

---

# Section 5: The Reliability Score as an N-Invariant Alternative

To build a universal lookup table mapping a metric to coverage loss, we need a measure that reflects the *severity* of misspecification independent of sample size. This section:

1. Introduces the **Raw Ratio** and demonstrates its superior N-stability compared to $S_{\mathrm{Inf}}$.
2. Derives a finite-sample correction based on Taylor expansion of the HC2 variance ratio.
3. Empirically estimates the optimal scaling factor and compares candidate corrections.
4. Formally tests which formulation achieves the best N-invariance.

## 5.0 Compute Ratio Metrics (Post-hoc)

Having established the N-dependence problem for the inferential score, we now compute ratio-based metrics as an alternative. The **Raw Ratio** inverts the inferential score's numerator and denominator:

$$ R = \frac{\text{SE}_{\text{classic}}}{\text{SE}_{\text{robust}}} $$

This metric is bounded (values near 1 indicate agreement between classical and robust SEs) and interpretable as a "reliability" measure: lower values indicate that robust SEs are inflated relative to classical SEs, signaling potential heteroskedasticity.

```{r compute-ratio-metrics}
# Add ratio metrics post-hoc using the stored SE values
# sr_ratio = se_classic / se_robust (Raw Ratio)
hetero_sim_results[, sr_ratio := se_classic / se_robust]

cat("Raw Ratio (sr_ratio = SE_classic / SE_robust) Summary:\n")
print(summary(hetero_sim_results$sr_ratio))
```

## 5.1 Raw Ratio vs Inferential Score: N-Dependence Comparison

We first compare the N-dependence of the **Raw Ratio** against the **Inferential Score** established in Section 4.

```{r ratio-vs-sinf-comparison, fig.width=12, fig.height=5}
# Use all lambda values for comprehensive comparison
subset_params <- hetero_sim_results[beta_x == 0.5 & sigma0 == 1.0]

# Aggregate both metrics
comparison_summary <- subset_params[, .(
  mean_T_sinf = mean(T_SInf),
  mean_sr_ratio = mean(sr_ratio)
), by = .(N, hetero_strength)]

# Plot T_SInf vs N (already shown to be N-dependent)
p1 <- ggplot(comparison_summary, aes(x = N, y = mean_T_sinf, color = as.factor(hetero_strength))) +
  geom_line(size = 0.8) +
  geom_point(size = 1.5) +
  scale_x_log10() +
  scale_color_viridis_d(option = "plasma", end = 0.9) +
  labs(
    title = expression("Scaled Inferential Score " * T[SInf] * " vs N"),
    subtitle = "Strong N-dependence: lines diverge with increasing N",
    y = expression("Mean " * T[SInf]),
    x = "N (log scale)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

# Plot Raw Ratio vs N
p2 <- ggplot(comparison_summary, aes(x = N, y = mean_sr_ratio, color = as.factor(hetero_strength))) +
  geom_line(size = 0.8) +
  geom_point(size = 1.5) +
  scale_x_log10() +
  scale_color_viridis_d(option = "plasma", end = 0.9) +
  labs(
    title = "Raw Ratio vs N",
    subtitle = "Much weaker N-dependence: lines remain approximately parallel",
    y = "Mean Raw Ratio",
    x = "N (log scale)",
    color = "λ"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

gridExtra::grid.arrange(p1, p2, ncol = 2, widths = c(0.45, 0.55))
```

**Key Observation:** The Raw Ratio exhibits dramatically less N-dependence than $T_{S_{\mathrm{Inf}}}$. While $T_{S_{\mathrm{Inf}}}$ shows lines that fan out (diverge) as N increases, the Raw Ratio lines remain approximately parallel across the N range. However, there is still residual finite-sample bias visible as a slight upward drift with N.

## 5.2 Theoretical Basis for Finite-Sample Correction

The residual N-dependence in the Raw Ratio arises from the finite-sample behavior of HC2. Under the null hypothesis (homoskedasticity), the HC2 standard error exhibits finite-sample bias that scales with $1/\sqrt{N}$.

### Finite-Sample Bias Structure

The HC2 variance estimator is:
$$ \hat{V}_{HC2} = (X'X)^{-1} \left( \sum_{i=1}^{n} \frac{\hat{e}_i^2}{1 - h_{ii}} x_i x_i' \right) (X'X)^{-1} $$

where $h_{ii}$ are the leverage values. Under homoskedasticity, we expect:
$$ E\left[\frac{\text{SE}_{\text{classic}}}{\text{SE}_{\text{robust}}}\right] \approx 1 + \frac{c}{\sqrt{N}} + O(N^{-1}) $$

for some constant $c$ that depends on the model structure. The **Reliability Score** corrects for this bias:

$$ S_{\mathrm{Rel}} = \frac{\text{SE}_{\text{classic}}}{\text{SE}_{\text{robust}}} - \frac{c}{\sqrt{N}} $$

We now estimate $c$ empirically from our simulation data.

## 5.3 Empirical Estimation of the Scaling Factor

We estimate the optimal scaling constant $c$ by regressing the Raw Ratio on $1/\sqrt{N}$ under the null ($\lambda = 0$).

```{r estimate-scaling-factor}
# Subset to null (lambda = 0) conditions only
null_data <- hetero_sim_results[hetero_strength == 0]

# Compute 1/sqrt(N)
null_data[, inv_sqrt_N := 1 / sqrt(N)]

# Regress raw ratio on 1/sqrt(N)
# sr_ratio = intercept + slope * (1/sqrt(N))
scaling_fit <- lm(sr_ratio ~ inv_sqrt_N, data = null_data)
scaling_summary <- summary(scaling_fit)

cat("Empirical Estimation of Scaling Factor\n")
cat("=" |> rep(60) |> paste(collapse = ""), "\n\n")
cat("Model: sr_ratio = intercept + c * (1/sqrt(N))\n")
cat("Under null (λ = 0), we expect intercept ≈ 1\n\n")

intercept_est <- coef(scaling_fit)[1]
slope_est <- coef(scaling_fit)[2]
c_empirical <- slope_est

cat(sprintf("Intercept:           %.4f (expected: 1.0)\n", intercept_est))
cat(sprintf("Slope (c):           %.4f\n", slope_est))
cat(sprintf("Empirical c:         %.4f\n", c_empirical))
cat(sprintf("\nR²:                  %.4f\n", scaling_summary$r.squared))
```

```{r scaling-visualization, fig.width=8, fig.height=5}
# Aggregate null data by N for cleaner visualization
null_summary <- null_data[, .(
  mean_sr_ratio = mean(sr_ratio),
  se_sr_ratio = sd(sr_ratio) / sqrt(.N)
), by = N]
null_summary[, inv_sqrt_N := 1 / sqrt(N)]

# Plot with empirical fit
ggplot(null_summary, aes(x = inv_sqrt_N, y = mean_sr_ratio)) +
  geom_point(size = 3, color = "steelblue") +
  geom_errorbar(aes(ymin = mean_sr_ratio - 1.96*se_sr_ratio, 
                    ymax = mean_sr_ratio + 1.96*se_sr_ratio), 
                width = 0.002, color = "steelblue") +
  # Empirical fit
  geom_abline(intercept = intercept_est, slope = slope_est, 
              color = "red", linetype = "solid", size = 1) +
  labs(
    title = "Finite-Sample Bias in Raw Ratio Under Null",
    subtitle = sprintf("Empirical fit: c = %.3f", c_empirical),
    x = expression(1/sqrt(N)),
    y = "Mean Raw Ratio"
  ) +
  theme_minimal() +
  annotate("text", x = 0.10, y = intercept_est + slope_est * 0.05, 
           label = sprintf("c = %.3f", c_empirical), color = "red", size = 5)
```

## 5.4 Investigation: Is the Scaling Factor a Degrees-of-Freedom Artifact?

The empirical scaling factor $c \approx 0.25$ may be related to model degrees of freedom. Since the ANOVA in Section 4.2 showed that $\beta_x$ and $\sigma_0$ have negligible effects on the scores, we can investigate whether $c$ depends on model complexity (number of parameters) by comparing:

1. **Simple regression**: $Y = \beta_0 + \beta_1 X + \varepsilon$ (2 parameters, df = N-2)
2. **Multiple regression**: $Y = \beta_0 + \beta_1 X + \beta_2 Z + \varepsilon$ (3 parameters, df = N-3)

If $c$ scales with df or the number of parameters, this would explain its origin and allow us to derive a general correction formula.

```{r df-artifact-investigation}
# Run dedicated simulations for simple vs multiple regression under null
N_test_grid <- c(50, 100, 200, 400, 800, 1600)
n_sims_test <- 1000

cat("Investigating df-dependence of scaling factor c\n")
cat("=" |> rep(60) |> paste(collapse = ""), "\n\n")

# Function to compute sr_ratio for a single simulation
compute_sr_ratio_simple <- function(N) {
  # Simple regression: y ~ x (2 parameters)
  x <- rnorm(N)
  y <- 1 + 0.5 * x + rnorm(N)  # Homoskedastic null
  fit <- lm(y ~ x)
  
  # Classical SE
  se_classic <- summary(fit)$coefficients["x", "Std. Error"]
  
  # HC2 SE
  vcov_hc2 <- sandwich::vcovHC(fit, type = "HC2")
  se_robust <- sqrt(diag(vcov_hc2))["x"]
  
  se_classic / se_robust
}

compute_sr_ratio_multiple <- function(N) {
  # Multiple regression: y ~ x + z (3 parameters)
  x <- rnorm(N)
  z <- rnorm(N)
  y <- 1 + 0.5 * x + 0.3 * z + rnorm(N)  # Homoskedastic null
  fit <- lm(y ~ x + z)
  
  # Classical SE for x coefficient
  se_classic <- summary(fit)$coefficients["x", "Std. Error"]
  
  # HC2 SE for x coefficient
  vcov_hc2 <- sandwich::vcovHC(fit, type = "HC2")
  se_robust <- sqrt(diag(vcov_hc2))["x"]
  
  se_classic / se_robust
}

# Run simulations for simple regression (p = 2)
cat("Running simulations for simple regression (y ~ x, p = 2)...\n")
simple_results <- data.table(
  N = rep(N_test_grid, each = n_sims_test),
  model = "Simple (p=2)",
  sr_ratio = NA_real_
)

for (i in 1:nrow(simple_results)) {
  simple_results$sr_ratio[i] <- compute_sr_ratio_simple(simple_results$N[i])
}

# Run simulations for multiple regression (p = 3)
cat("Running simulations for multiple regression (y ~ x + z, p = 3)...\n")
multiple_results <- data.table(
  N = rep(N_test_grid, each = n_sims_test),
  model = "Multiple (p=3)",
  sr_ratio = NA_real_
)

for (i in 1:nrow(multiple_results)) {
  multiple_results$sr_ratio[i] <- compute_sr_ratio_multiple(multiple_results$N[i])
}

# Combine results
df_test_results <- rbind(simple_results, multiple_results)
df_test_results[, inv_sqrt_N := 1 / sqrt(N)]

cat("Done.\n\n")
```

```{r fit-scaling-by-model}
# Fit scaling factor for each model type
simple_fit <- lm(sr_ratio ~ inv_sqrt_N, data = df_test_results[model == "Simple (p=2)"])
multiple_fit <- lm(sr_ratio ~ inv_sqrt_N, data = df_test_results[model == "Multiple (p=3)"])

c_simple <- coef(simple_fit)[2]
c_multiple <- coef(multiple_fit)[2]
intercept_simple <- coef(simple_fit)[1]
intercept_multiple <- coef(multiple_fit)[1]

cat("Scaling Factor Comparison by Model Complexity\n")
cat("=" |> rep(60) |> paste(collapse = ""), "\n\n")

cat(sprintf("%-25s %12s %12s %12s\n", "Model", "Intercept", "c (slope)", "R²"))
cat("-" |> rep(60) |> paste(collapse = ""), "\n")
cat(sprintf("%-25s %12.4f %12.4f %12.4f\n", 
            "Simple (y ~ x, p=2)", 
            intercept_simple, c_simple, summary(simple_fit)$r.squared))
cat(sprintf("%-25s %12.4f %12.4f %12.4f\n", 
            "Multiple (y ~ x + z, p=3)", 
            intercept_multiple, c_multiple, summary(multiple_fit)$r.squared))

cat("\n\nInterpretation:\n")
cat("-" |> rep(50) |> paste(collapse = ""), "\n")

# Check if c scales with p
ratio_c <- c_multiple / c_simple
cat(sprintf("Ratio c_multiple / c_simple: %.3f\n", ratio_c))
cat(sprintf("Expected if c ~ p:          %.3f (3/2 = 1.5)\n", 3/2))
cat(sprintf("Expected if c ~ (p-1):      %.3f (2/1 = 2.0)\n", 2/1))

# Estimate c per parameter
c_per_param_simple <- c_simple / 2
c_per_param_multiple <- c_multiple / 3
cat(sprintf("\nc per parameter (simple):   %.4f\n", c_per_param_simple))
cat(sprintf("c per parameter (multiple): %.4f\n", c_per_param_multiple))
```

```{r df-comparison-plot, fig.width=10, fig.height=5}
# Aggregate by N and model
df_summary <- df_test_results[, .(
  mean_sr_ratio = mean(sr_ratio),
  se_sr_ratio = sd(sr_ratio) / sqrt(.N)
), by = .(N, model, inv_sqrt_N)]

# Plot comparison
ggplot(df_summary, aes(x = inv_sqrt_N, y = mean_sr_ratio, color = model)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean_sr_ratio - 1.96*se_sr_ratio, 
                    ymax = mean_sr_ratio + 1.96*se_sr_ratio), 
                width = 0.003) +
  # Fit lines
  geom_abline(intercept = intercept_simple, slope = c_simple, 
              color = "steelblue", linetype = "solid", size = 1) +
  geom_abline(intercept = intercept_multiple, slope = c_multiple, 
              color = "coral", linetype = "solid", size = 1) +
  scale_color_manual(values = c("Simple (p=2)" = "steelblue", "Multiple (p=3)" = "coral")) +
  labs(
    title = "Scaling Factor Depends on Model Complexity",
    subtitle = sprintf("Simple: c = %.3f | Multiple: c = %.3f", c_simple, c_multiple),
    x = expression(1/sqrt(N)),
    y = "Mean Raw Ratio",
    color = "Model"
  ) +
  theme_minimal() +
  theme(legend.position = "right")
```

```{r df-conclusion}
cat("\n\nConclusion: Degrees of Freedom Artifact\n")
cat("=" |> rep(60) |> paste(collapse = ""), "\n\n")

cat("The scaling factor c varies with model complexity (number of parameters p).\n")
cat("This suggests the finite-sample bias is related to degrees of freedom.\n\n")

# Use the empirical c from our main simulation (simple regression)
cat(sprintf("For our simple regression setup (y ~ x):\n"))
cat(sprintf("  Empirical c from main simulation: %.4f\n", c_empirical))
cat(sprintf("  Empirical c from dedicated test:  %.4f\n", c_simple))
cat(sprintf("\nWe will use c = %.4f for the Reliability Score correction.\n", c_empirical))
```

## 5.5 Comparison of Scaling Factors for N-Invariance

We now compare the raw ratio against the empirically-adjusted ratio:

1. **Raw Ratio** (no correction): $R = \text{SE}_{\text{classic}} / \text{SE}_{\text{robust}}$
2. **Empirical correction** (Reliability Score): $S_{\mathrm{Rel}} = R - c/\sqrt{N}$ where $c$ is the empirically estimated value

```{r compare-scaling-factors}
# Compute adjusted ratio using empirical c
hetero_sim_results[, sr_ratio_adj := sr_ratio - c_empirical / sqrt(N)]

# Aggregate by N and lambda
scaling_comparison <- hetero_sim_results[, .(
  mean_raw = mean(sr_ratio),
  mean_adjusted = mean(sr_ratio_adj)
), by = .(N, hetero_strength)]

# Compute spread (max - min across N) for each lambda and metric
spread_by_lambda <- scaling_comparison[, .(
  spread_raw = max(mean_raw) - min(mean_raw),
  spread_adjusted = max(mean_adjusted) - min(mean_adjusted)
), by = hetero_strength]

cat("N-Invariance Comparison: Spread (max - min) Across Sample Sizes\n")
cat("=" |> rep(60) |> paste(collapse = ""), "\n\n")
cat(sprintf("Empirical scaling factor c = %.4f\n\n", c_empirical))
cat(sprintf("%-10s %15s %15s\n", "Lambda", "Raw Ratio", "Adjusted (S_Rel)"))
cat("-" |> rep(60) |> paste(collapse = ""), "\n")

for (i in 1:nrow(spread_by_lambda)) {
  cat(sprintf("%-10.1f %15.4f %15.4f\n",
              spread_by_lambda$hetero_strength[i],
              spread_by_lambda$spread_raw[i],
              spread_by_lambda$spread_adjusted[i]))
}

# Summary statistics
cat("\n")
cat("-" |> rep(60) |> paste(collapse = ""), "\n")
cat(sprintf("%-10s %15.4f %15.4f\n", "Mean",
            mean(spread_by_lambda$spread_raw),
            mean(spread_by_lambda$spread_adjusted)))
cat(sprintf("%-10s %15.4f %15.4f\n", "Max",
            max(spread_by_lambda$spread_raw),
            max(spread_by_lambda$spread_adjusted)))

# Improvement
improvement <- (mean(spread_by_lambda$spread_raw) - mean(spread_by_lambda$spread_adjusted)) / 
               mean(spread_by_lambda$spread_raw) * 100
cat(sprintf("\nImprovement in mean spread: %.1f%%\n", improvement))
```

```{r scaling-comparison-plot, fig.width=10, fig.height=4}
# Melt for plotting
scaling_long <- melt(scaling_comparison, 
                     id.vars = c("N", "hetero_strength"),
                     measure.vars = c("mean_raw", "mean_adjusted"),
                     variable.name = "Metric", value.name = "Value")

scaling_long[, Metric := factor(Metric, 
                                levels = c("mean_raw", "mean_adjusted"),
                                labels = c("Raw Ratio", 
                                          sprintf("Reliability Score (c=%.3f)", c_empirical)))]

# Use all lambda values
ggplot(scaling_long, aes(x = N, y = Value, color = as.factor(hetero_strength))) +
  geom_line(size = 0.8) +
  geom_point(size = 1.5) +
  scale_x_log10() +
  scale_color_viridis_d(option = "plasma", end = 0.9) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(
    title = "Comparison: Raw Ratio vs Empirically-Adjusted Reliability Score",
    subtitle = "Flatter lines indicate better N-invariance",
    x = "Sample Size (log scale)",
    y = "Score Value",
    color = "λ"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

## 5.6 Formal ANOVA Test: Raw Ratio vs Adjusted Scores

We now apply the same ANOVA framework from Section 4.2 to formally test N-dependence for each metric. Using raw simulation data (~300k observations), we compute η² to assess effect sizes.

```{r anova-raw-ratio}
cat("ANOVA on Raw Ratio: sr_ratio ~ N × λ × β_x × σ0\n")
cat("=" |> rep(70) |> paste(collapse = ""), "\n\n")

anova_raw <- aov(sr_ratio ~ N_factor * lambda_factor * beta_factor * sigma_factor, 
                 data = hetero_sim_results)
anova_raw_summary <- summary(anova_raw)

# Compute eta-squared
ss_total_raw <- sum(anova_raw_summary[[1]]$`Sum Sq`)
anova_raw_table <- anova_raw_summary[[1]]
anova_raw_table$eta_sq <- anova_raw_table$`Sum Sq` / ss_total_raw

# Extract key effects
n_eta_raw <- anova_raw_table["N_factor", "eta_sq"]
lambda_eta_raw <- anova_raw_table["lambda_factor", "eta_sq"]
n_lambda_eta_raw <- anova_raw_table["N_factor:lambda_factor", "eta_sq"]

cat(sprintf("N Effect:           η² = %.4f (%.2f%%)\n", n_eta_raw, n_eta_raw * 100))
cat(sprintf("λ Effect:           η² = %.4f (%.2f%%)\n", lambda_eta_raw, lambda_eta_raw * 100))
cat(sprintf("N × λ Interaction:  η² = %.4f (%.2f%%)\n", n_lambda_eta_raw, n_lambda_eta_raw * 100))
```

```{r anova-reliability-score}
cat("\n\nANOVA on Reliability Score: sr_ratio_adj ~ N × λ × β_x × σ0\n")
cat("=" |> rep(70) |> paste(collapse = ""), "\n\n")

anova_adj <- aov(sr_ratio_adj ~ N_factor * lambda_factor * beta_factor * sigma_factor, 
                 data = hetero_sim_results)
anova_adj_summary <- summary(anova_adj)

# Compute eta-squared
ss_total_adj <- sum(anova_adj_summary[[1]]$`Sum Sq`)
anova_adj_table <- anova_adj_summary[[1]]
anova_adj_table$eta_sq <- anova_adj_table$`Sum Sq` / ss_total_adj

# Extract key effects
n_eta_adj <- anova_adj_table["N_factor", "eta_sq"]
lambda_eta_adj <- anova_adj_table["lambda_factor", "eta_sq"]
n_lambda_eta_adj <- anova_adj_table["N_factor:lambda_factor", "eta_sq"]

cat(sprintf("N Effect:           η² = %.4f (%.2f%%)\n", n_eta_adj, n_eta_adj * 100))
cat(sprintf("λ Effect:           η² = %.4f (%.2f%%)\n", lambda_eta_adj, lambda_eta_adj * 100))
cat(sprintf("N × λ Interaction:  η² = %.4f (%.2f%%)\n", n_lambda_eta_adj, n_lambda_eta_adj * 100))
```

```{r anova-comparison-summary}
cat("\n\n")
cat("=" |> rep(70) |> paste(collapse = ""), "\n")
cat("SUMMARY: N-Effect Comparison (η²)\n")
cat("=" |> rep(70) |> paste(collapse = ""), "\n\n")

cat(sprintf("%-25s %12s %12s %12s\n", "Metric", "N Effect", "λ Effect", "N×λ Inter."))
cat("-" |> rep(70) |> paste(collapse = ""), "\n")
cat(sprintf("%-25s %11.4f%% %11.4f%% %11.4f%%\n", 
            "T_SInf (Inferential)", n_eta * 100, lambda_eta * 100, n_lambda_eta * 100))
cat(sprintf("%-25s %11.4f%% %11.4f%% %11.4f%%\n", 
            "Raw Ratio", n_eta_raw * 100, lambda_eta_raw * 100, n_lambda_eta_raw * 100))
cat(sprintf("%-25s %11.4f%% %11.4f%% %11.4f%%\n", 
            sprintf("Reliability Score (c=%.2f)", c_empirical), 
            n_eta_adj * 100, lambda_eta_adj * 100, n_lambda_eta_adj * 100))

cat("\n\nInterpretation:\n")
cat("-" |> rep(50) |> paste(collapse = ""), "\n")

# Calculate improvement
improvement_raw_vs_sinf <- (n_eta - n_eta_raw) / n_eta * 100
improvement_adj_vs_raw <- (n_eta_raw - n_eta_adj) / n_eta_raw * 100

cat(sprintf("→ Raw Ratio reduces N-effect by %.1f%% compared to T_SInf\n", improvement_raw_vs_sinf))
cat(sprintf("→ Reliability Score reduces N-effect by %.1f%% compared to Raw Ratio\n", improvement_adj_vs_raw))

if (n_eta_adj < 0.01) {
  cat("→ Reliability Score N-effect is NEGLIGIBLE (η² < 1%)\n")
  cat(sprintf("→ The empirical correction (c = %.3f) achieves approximate N-invariance.\n", c_empirical))
} else if (n_eta_adj < 0.06) {
  cat("→ Reliability Score N-effect is SMALL (η² < 6%)\n")
}
```

## 5.6 Classical Coverage Degradation

We visualize how classical CI coverage degrades with increasing heteroskedasticity. This motivates using the Reliability Score to map to coverage loss.

```{r coverage-degradation, fig.width=10, fig.height=5}
# Aggregate coverage by N and hetero_strength
coverage_summary <- hetero_sim_results[, .(
  mean_coverage = mean(coverage) * 100,
  se_coverage = sd(coverage) / sqrt(.N) * 100
), by = .(N, hetero_strength)]

ggplot(coverage_summary, aes(x = hetero_strength, y = mean_coverage, color = as.factor(N))) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  geom_hline(yintercept = 95, linetype = "dashed", color = "gray50") +
  labs(
    title = "Classical CI Coverage Degradation Under Heteroskedasticity",
    subtitle = "Coverage of classical Wald intervals (using SE_classic)",
    x = "Heteroskedasticity Strength (lambda)",
    y = "Coverage (%)",
    color = "Sample Size (N)"
  ) +
  theme_minimal() +
  coord_cartesian(ylim = c(50, 100))
```

**Key Observation:** Classical coverage degrades systematically with heteroskedasticity. The Reliability Score $S_{\mathrm{Rel}}$ can be mapped to this coverage loss because it is N-invariant—unlike $S_{\mathrm{Inf}}$, which conflates severity with sample size.

---

# Section 6: Benchmark Against Standard Heteroskedasticity Tests

We benchmark the detection capability of our scores against standard tests (Breusch-Pagan and White). We run this on a subset of the grid to manage computational load.

```{r benchmark-tests}
# Define benchmark subset
N_bench <- c(100, 400, 1600)
lambda_bench <- seq(0, 2, by = 0.2)
n_reps_bench <- 200

# Function to run benchmarks
run_benchmark <- function(N, lambda) {
  reps <- replicate(n_reps_bench, {
    # Generate data
    # Using the exact logic from the main grid for consistency
    x <- rnorm(N)
    var_i <- 1 + lambda * x^2
    y <- 1 + 0.5 * x + rnorm(N, 0, sqrt(var_i))
    
    # Fit LM
    fit <- lm(y ~ x)
    
    # Tests
    bp_pval <- lmtest::bptest(fit)$p.value
    # White test (BP on squares)
    white_pval <- lmtest::bptest(fit, ~ x + I(x^2))$p.value
    
    # S_Inf Test
    metrics <- compute_se_metrics(fit, coef_name = "x", hc_type = best_hc)
    # metrics$sr_inf is the unscaled score. We need T_SInf = sr_inf_adj
    t_sinf <- metrics$sr_inf_adj
    
    c(
      reject_bp = bp_pval < 0.05,
      reject_white = white_pval < 0.05,
      reject_sinf = t_sinf > 1.645
    )
  })
  rowMeans(reps)
}

# Run benchmark grid
benchmark_res <- expand.grid(N = N_bench, lambda = lambda_bench)
benchmark_res$reject_bp <- NA
benchmark_res$reject_white <- NA
benchmark_res$reject_sinf <- NA

# Loop (simple for loop for benchmark)
for(i in 1:nrow(benchmark_res)) {
  res <- run_benchmark(benchmark_res$N[i], benchmark_res$lambda[i])
  benchmark_res$reject_bp[i] <- res["reject_bp"]
  benchmark_res$reject_white[i] <- res["reject_white"]
  benchmark_res$reject_sinf[i] <- res["reject_sinf"]
}

# Reshape for plotting
bench_long <- melt(as.data.table(benchmark_res), id.vars = c("N", "lambda"), 
                   variable.name = "Test", value.name = "Power")

ggplot(bench_long, aes(x = lambda, y = Power, color = Test)) +
  geom_line() +
  facet_wrap(~N) +
  labs(title = "Power Comparison: S_Inf vs Standard Tests",
       y = "Rejection Rate (alpha=0.05)") +
  theme_minimal()
```

**Interpretation:**
- $S_{\mathrm{Inf}}$ performs comparably to standard tests (BP, White) in detecting heteroskedasticity.
- However, unlike p-values from BP/White, the **Reliability Score** derived from $S_{\mathrm{Inf}}$ provides a calibrated *measure of severity* that maps directly to coverage loss, which is our primary goal.

---

# Section 7: Coverage-Based Mapping and Universal Lookup

We now construct the final lookup table mapping the Reliability Score to expected coverage loss.

## 7.1 Aggregation

```{r hetero-aggregation}
# Aggregate simulation results by sample size, heteroskedasticity strength, beta, and sigma0
# Note: This aggregates sr_inf, sr_inf_adj metrics from simulation
aggregated_hetero <- aggregate_hetero_sims(hetero_sim_results)

# Add aggregated ratio metrics post-hoc (computed in Section 5)
aggregated_hetero <- add_aggregated_ratio_metrics(aggregated_hetero, hetero_sim_results)

# Save aggregated results
save_rds(aggregated_hetero, "hetero_aggregated.rds")

cat(sprintf("Aggregated to %d rows (one per cell of experimental grid)\n\n", nrow(aggregated_hetero)))
```

## 7.2 Coverage-Adjusted Lookup Table

We build a wide-format table showing the relationship between coverage gap and reliability score across sample sizes.

```{r coverage-lookup}
# Build wide-format lookup indexed by coverage gap and sample size
coverage_table_result <- build_coverage_adjusted_table(aggregated_hetero)

wide_table <- coverage_table_result$wide_table
coverage_gaps <- coverage_table_result$coverage_gaps
N_values <- coverage_table_result$N_values

# Save comprehensive lookup structure
save_rds(coverage_table_result, "coverage_adjusted_by_N.rds")
```

## 7.3 N-Invariance Analysis

We formally test the N-invariance of the Reliability Score using three approaches.

### Spread Analysis
Range (max - min) of reliability scores for each coverage gap across N.

```{r spread-analysis}
# Compute spread per coverage gap
spread_table <- compute_spread_per_coverage(wide_table)

cat("Spread (max − min of reliability score) per coverage gap:\n\n")
print(spread_table)
```

### Regression Analysis
Slope of Reliability Score ~ log(N) for each coverage stratum.

```{r regression-analysis}
# Fit stratum-specific regressions
regression_table <- run_per_coverage_regressions(aggregated_hetero)

cat("\nRegression: Reliability Score ~ log(N) by coverage gap\n")
cat("(Slope near 0 and p > 0.05 indicate N-invariance)\n\n")
print(regression_table)
```

### Mixed-Effects Model
Global test for N-dependence accounting for coverage strata.

```{r mixed-model-analysis}
# Fit global mixed model with coverage gap as random intercept
mixed_result <- fit_global_mixed_model(aggregated_hetero)

cat("\nMixed-Effects Model: Reliability Score ~ log(N) + (1 | coverage_gap)\n\n")
cat(mixed_result$summary_text)
cat(sprintf("\nGlobal slope:        %.6f\n", mixed_result$slope))
cat(sprintf("P-value:             %.4f\n", mixed_result$p_value))
cat(sprintf("Total N-effect:      %.4f\n\n", mixed_result$total_effect))
```

## 7.4 Universal Lookup Table

Construct the final universal lookup table by averaging the reliability score across the sample size range for each coverage gap.

```{r universal-lookup}
# Build universal lookup: mean reliability score for each coverage gap (aggregated over N)
universal_lookup <- build_universal_lookup_table(aggregated_hetero)

# Save lookup table in multiple formats
save_rds(universal_lookup, "universal_lookup_table.rds")
save_table_csv(universal_lookup, "universal_lookup_table.csv")

cat("Universal Lookup Table (mean reliability score by coverage gap):\n\n")
print(universal_lookup)
```

## Visualization

```{r lookup-plot, fig.width=10, fig.height=6}
# Plot final universal lookup
p_lookup <- ggplot(
  universal_lookup,
  aes(x = coverage_gap_pct, y = universal_sr_ratio_adj)
) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  labs(
    title = sprintf("Universal %s Lookup Table", best_hc),
    x = "Empirical Coverage Gap (%)",
    y = "Reliability Score"
  ) +
  theme_minimal()

print(p_lookup)
```

---

# Summary

This validation workflow has established:

1.  **Power**: The inferential score $S_{\mathrm{Inf}}$ (based on `r best_hc`) has high power to detect heteroskedasticity, comparable to standard tests like Breusch-Pagan and White.
2.  **N-Dependence**: Like any test statistic, $S_{\mathrm{Inf}}$ scales with $\sqrt{N}$, making it unsuitable as a direct measure of misspecification severity across different sample sizes.
3.  **Reliability Score**: The Reliability Score $S_{\mathrm{Rel}}$ removes this N-dependence. It is approximately invariant to sample size, effect size ($\beta_x$), and baseline noise ($\sigma_0$).
4.  **Universal Mapping**: We have constructed a universal lookup table that maps $S_{\mathrm{Rel}}$ directly to expected coverage loss. This allows applied researchers to diagnose the reliability of their inference using a single metric.

## Reproducibility Notes

- Random seeds set immediately before each randomization-dependent function call.
- All configuration parameters documented in `R/00_config.R`.
- Intermediate results saved at each analytical step in `results/`.
- Complete dataset reproducible by re-running this notebook with identical seed and configuration.
