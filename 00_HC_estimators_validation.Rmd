---
title: "Part 0: Validation of Matrix-Based OLS and HC Estimators"
author: "Quantitative Methods"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Load project configuration and functions
source("R/00_config.R")
source("R/10_dgp_and_fits.R")
```

# 1. Purpose

The purpose of this document is to rigorously validate the fast, matrix-based implementation of Ordinary Least Squares (OLS) and Heteroskedasticity-Consistent (HC) covariance estimators used in this project.

**Scope**:
*   This document validates the **implementation** of OLS and HC1–HC3.
*   It does NOT study coverage or $S_{\text{Inf}}$ behavior; those are addressed in `01_null_calibration.Rmd` and subsequent notebooks.

**Motivation**:
Later notebooks use a high-performance matrix engine instead of repeatedly calling `lm()` and `vcovHC()`. We must verify that this engine is **algebraically correct** and **numerically indistinguishable** from the well-vetted R implementations (`stats::lm`, `sandwich::vcovHC`).

**Reproducibility and Tolerances**:
*   All simulations use a fixed global random seed (2024) set once in `R/00_config.R` and inherited by this notebook via `source("R/00_config.R")`.
*   When we compare numerical implementations, differences on the order of $10^{-10}$ or smaller are treated as negligible, consistent with double-precision floating-point rounding error.

# 2. Theoretical Equivalence

## 2.1 OLS Estimator

For a linear regression model $y = X\beta + \varepsilon$, where $y$ is an $n \times 1$ vector, $X$ is an $n \times k$ design matrix (including a column of ones for the intercept), and $\beta$ is a $k \times 1$ coefficient vector, the OLS estimator is:

$$ \hat{\beta} = (X'X)^{-1} X'y $$

The residuals are $\hat{u} = y - X\hat{\beta}$. The classic homoskedastic covariance matrix is:

$$ \widehat{\text{Var}}_{\text{OLS}}(\hat{\beta}) = \hat{\sigma}^2 (X'X)^{-1}, \quad \text{where } \hat{\sigma}^2 = \frac{\sum \hat{u}_i^2}{n - k} $$

These are the standard least-squares formulas; see, for example, Greene (2012, Ch. 3).

## 2.2 HC Estimators (Sandwich Form)

The Heteroskedasticity-Consistent covariance matrix takes the "sandwich" form (White, 1980):

$$ \widehat{\text{Var}}_{\text{HC}}(\hat{\beta}) = (X'X)^{-1} X' \hat{\Omega} X (X'X)^{-1} $$

where $\hat{\Omega} = \text{diag}(\omega_1, \dots, \omega_n)$ is a diagonal matrix of weighted squared residuals. The weights $\omega_i$ differ by HC type (MacKinnon & White, 1985; Long & Ervin, 2000):

*   **HC0**: $\omega_i = \hat{u}_i^2$
*   **HC1**: $\omega_i = \frac{n}{n-k} \hat{u}_i^2$ (Degrees of freedom adjustment)
*   **HC2**: $\omega_i = \frac{\hat{u}_i^2}{1 - h_{ii}}$ (Leverage adjustment)
*   **HC3**: $\omega_i = \frac{\hat{u}_i^2}{(1 - h_{ii})^2}$ (Jackknife approximation)

Here, $h_{ii}$ are the diagonal elements of the hat matrix $H = X(X'X)^{-1}X'$.

**Note**: For HC2 and HC3 we divide by $(1 - h_{ii})$ and $(1 - h_{ii})^2$. Under standard OLS assumptions with an intercept and $n > k$, all leverages satisfy $0 < h_{ii} < 1$, so these denominators are well-defined. If the design is rank-deficient, our core solver will stop with an error.

## 2.3 Matrix Implementation

Our fast implementation (`fit_ols_hc` in `R/10_dgp_and_fits.R`) computes these quantities directly using optimized linear algebra:

*   $(X'X)^{-1}$ $\to$ `XtX_inv <- chol2inv(chol(crossprod(X)))` (Cholesky decomposition).
*   $\hat{\beta}$ $\to$ `beta <- backsolve(R, forwardsolve(t(R), crossprod(X, y)))`.
*   Residuals $\hat{u}$ $\to$ `resid <- y - X %*% beta`.
*   Leverages $h_{ii}$ $\to$ `h <- rowSums((X %*% XtX_inv) * X)`.
*   $\hat{\Omega}$ via $\omega_i$ $\to$ implemented via `X_weighted <- X * sqrt(u_sq)`; `meat <- crossprod(X_weighted)` ($= X' \hat{\Omega} X$).

This approach avoids the overhead of creating S3 objects and method dispatch, providing significant speedups while maintaining algebraic exactness.

# 3. Numerical Validation Design

We define a grid of scenarios to stress-test the implementation across different sample sizes, dimensions, and data generating processes (DGPs).

## 3.1 Scenarios

*   **Sample Sizes ($N$)**: 20, 50, 100, 1000.
*   **Regressors ($p$)**: 1 (Simple Regression), 4 (Multiple Regression).
*   **DGPs**:
    1.  **Homoskedastic**:
        *   $X_j \sim N(0, 1)$ independently for $j=1,\dots,p$.
        *   $y = 1 + X\beta + \varepsilon$, with $\beta_j = 1$ and $\varepsilon \sim N(0, 1)$.
    2.  **Heteroskedastic**:
        *   Same $X$.
        *   $\varepsilon_i \sim N(0, \sigma_i^2)$, with
            \[
              \sigma_i = 1 + 0.5 X_{1,i}^2.
            \]
    3.  **Collinear** (only when $p > 1$):
        *   Draw $X_1 \sim N(0, 1)$.
        *   Set $X_2 = X_1 + \eta$ with $\eta \sim N(0, 0.01^2)$ to introduce near-collinearity.

## 3.2 Validation Logic

For each simulation and scenario, we validate:
1.  **OLS point estimates** $\hat{\beta}$.
2.  **Classic homoskedastic SEs** via `vcov()`.
3.  **HC1–HC3 robust SEs** via `vcovHC()`.

**Procedure**:
1.  Generate data $(y, X)$.
2.  **Reference**: Fit using `lm()`, compute SEs using `vcov()` and `sandwich::vcovHC()`.
3.  **Manual**: Compute $\hat{\beta}$ and SEs using the project's core matrix implementation `fit_ols_hc()`.
    *   *Note*: For $p=1$, we also cross-check the helper `fit_ols_get_hc_ci()` to ensure it wraps the core engine correctly.
4.  Compute **Absolute Difference** ($|Est_{ref} - Est_{man}|$) and **Relative Difference** ($|Est_{ref} - Est_{man}| / |Est_{ref}|$).

```{r run-validation}
# Simulation Parameters
N_vals <- c(20, 50, 100, 1000)
p_vals <- c(1, 4)
dgp_types <- c("Homoskedastic", "Heteroskedastic", "Collinear")
n_sims <- 200

results_list <- list()
counter <- 1

# Note: Global seed is set in R/00_config.R via source() in setup chunk

for (N in N_vals) {
  for (p in p_vals) {
    for (dgp in dgp_types) {
      
      for (i in 1:n_sims) {
        # 1. Generate Data
        X_raw <- matrix(rnorm(N * p), ncol = p)
        colnames(X_raw) <- paste0("x", 1:p)
        
        # Collinearity injection
        if (dgp == "Collinear" && p > 1) {
          X_raw[, 2] <- X_raw[, 1] + rnorm(N, 0, 0.01)
        }
        
        # Error term
        if (dgp == "Heteroskedastic") {
          eps <- rnorm(N, 0, 1 + 0.5 * X_raw[, 1]^2)
        } else {
          eps <- rnorm(N, 0, 1)
        }
        
        beta_true <- rep(1, p)
        y <- 1 + X_raw %*% beta_true + eps
        
        df <- data.frame(y = y, X_raw)
        
        # 2. Reference (lm + sandwich)
        fit_lm <- lm(y ~ ., data = df)
        beta_ref <- coef(fit_lm)
        se_classic_ref <- sqrt(diag(vcov(fit_lm)))
        se_hc1_ref <- sqrt(diag(vcovHC(fit_lm, type = "HC1")))
        se_hc2_ref <- sqrt(diag(vcovHC(fit_lm, type = "HC2")))
        se_hc3_ref <- sqrt(diag(vcovHC(fit_lm, type = "HC3")))
        
        # 3. Manual Implementation (Project Core)
        X_mat <- model.matrix(fit_lm)
        
        # Call the core engine directly
        manual_res <- fit_ols_hc(X_mat, y, hc_types = c("HC1", "HC2", "HC3"), check_rank = TRUE)
        
        # Cross-check for p=1 helper
        if (p == 1) {
          df_proj <- data.frame(y = y, x = X_raw[, 1])
          proj_res_hc2 <- fit_ols_get_hc_ci(df_proj, hc_type = "HC2")
          
          # Verify project function matches core engine for slope (index 2)
          diff_proj <- abs(proj_res_hc2$se_robust - manual_res$se_robust[["HC2"]][2])
          if (diff_proj > 1e-14) warning("Project function mismatch!")
        }
        
        # 4. Compare
        # Helper to store diffs
        store_diff <- function(metric, ref, man) {
          abs_d <- abs(ref - man)
          rel_d <- abs_d / (abs(ref) + 1e-100)
          data.table(
            N = N, p = p, dgp = dgp, sim = i,
            metric = metric,
            max_abs_diff = max(abs_d),
            max_rel_diff = max(rel_d)
          )
        }
        
        results_list[[counter]] <- rbind(
          store_diff("Beta", beta_ref, manual_res$beta),
          store_diff("SE_Classic", se_classic_ref, manual_res$se_classic),
          store_diff("SE_HC1", se_hc1_ref, manual_res$se_robust[["HC1"]]),
          store_diff("SE_HC2", se_hc2_ref, manual_res$se_robust[["HC2"]]),
          store_diff("SE_HC3", se_hc3_ref, manual_res$se_robust[["HC3"]])
        )
        counter <- counter + 1
      }
    }
  }
}

validation_dt <- rbindlist(results_list)
```

# 4. Numerical Results

We summarize the maximum absolute and relative differences observed across all simulations.

## 4.1 Summary by Metric

The table below shows the **worst-case** discrepancies encountered across all scenarios (N, p, DGP).

```{r summary-table}
summary_table <- validation_dt[, .(
  Max_Abs_Diff = max(max_abs_diff),
  Max_Rel_Diff = max(max_rel_diff)
), by = metric]

kable(summary_table, digits = 20, caption = "Maximum Differences: Manual vs Reference Implementation")
```

For **Beta** and **SE_Classic**, worst-case absolute differences are about $10^{-9}$ to $10^{-14}$ with relative differences around $10^{-11}$ to $10^{-15}$. For **HC1–HC3**, absolute differences are similarly small and relative differences are around $10^{-10}$. These are vastly smaller than any practical tolerance and consistent with numerical rounding when solving linear systems (especially given the Cholesky vs. QR differences in `lm`).

## 4.2 Distribution of Errors

We visualize the distribution of relative errors (log10 scale) to confirm that the vast majority of discrepancies are at machine precision ($\approx 10^{-16}$).

```{r error-plot, fig.height=6, fig.width=8}
ggplot(validation_dt, aes(x = log10(max_rel_diff + 1e-20))) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~ metric, scales = "free_y") +
  labs(
    title = "Distribution of Relative Differences (Log10 Scale)",
    subtitle = "Values near -16 indicate machine precision agreement",
    x = "Log10(Relative Difference)",
    y = "Count"
  ) +
  theme_minimal()
```

The histograms of log10(relative differences) cluster tightly between -15 and -13 for all metrics, indicating that manual and reference implementations agree at or near double-precision machine accuracy.

# 5. Conclusion

## 5.1 Theoretical Verification
Our matrix implementation follows the standard OLS and HC1–HC3 sandwich formulas (White, 1980; MacKinnon and White, 1985; Long and Ervin, 2000).

## 5.2 Numerical Verification
Across all scenarios ($N \in \{20, 50, 100, 1000\}$, $p \in \{1, 4\}$, with homoskedastic, heteroskedastic, and collinear designs), the maximum absolute difference between the manual estimates and the reference `lm`/`sandwich` estimates is on the order of $10^{-9}$ to $10^{-14}$, with relative differences around $10^{-11}$–$10^{-10}$.

## 5.3 OLS Verification
Manual OLS coefficients and classic covariance match `lm()` and `vcov()` to within machine precision.

**Implication**: Thus both the underlying OLS engine and the HC adjustments can be treated as numerically equivalent drop-in replacements for `lm()` + `vcov()`/`vcovHC()`, justifying their use in the large-scale simulations and calibration exercises in subsequent notebooks.

<!--
tests/testthat/test_hc_equivalence.R:
  * construct a few fixed X, y designs;
  * compare lm()/vcovHC() vs manual fit_ols_hc();
  * use expect_equal(..., tolerance = 1e-8) to allow for BLAS variation.
-->
